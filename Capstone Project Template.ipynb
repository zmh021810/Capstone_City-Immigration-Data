{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Scope:\n",
    "This project will pull data from all sources and create fact and dimension tables to show movement of other contries's person into US port city and the information of the population (sum, male, female) in this city.\n",
    "\n",
    "#### data source:\n",
    "\n",
    "- us-cities-demographics: comes from OpenSoft. It contains information about the demographics of all US cities and census-designated places with a population\n",
    "- sas_data: comes from the US National Tourism and Trade Office, it has immigrants and the ports city which they arrive.\n",
    "- countries: comes from I94_SAS_Labels_Descriptions.SAS\n",
    "- visa: comes from I94_SAS_Labels_Descriptions.SAS\n",
    "- I94 mode: comes from I94_SAS_Labels_Descriptions.SAS\n",
    "- airport: comes from datahub.io,  it has airport information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "with open('I94_SAS_Labels_Descriptions.SAS') as labels_descriptions:\n",
    "    raw_labels = labels_descriptions.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract country data\n",
    "labels = raw_labels[raw_labels.index(\"I94CIT & I94RES - This format shows all the valid and invalid codes for processing\"):]\n",
    "labels = labels[:labels.index(';')]\n",
    "lines = labels.splitlines()\n",
    "fields = ['code', 'Country']\n",
    "rows=[]\n",
    "for i in range(2,len(lines)):\n",
    "    rows.append(lines[i][1:].split(\"=\"))\n",
    "with open('countries.csv', 'w') as f:\n",
    "    write = csv.writer(f) \n",
    "    write.writerow(fields)\n",
    "    write.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read city data\n",
    "cities_df=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(\"us-cities-demographics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|            City|         State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race| Count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino| 25924|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White| 58723|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian|  4759|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...| 24437|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White| 76402|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...|  1343|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...| 11592|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian| 32716|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino|  2583|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian| 11060|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|  Hispanic or Latino|  5822|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|American Indian a...|   998|\n",
      "|    Philadelphia|  Pennsylvania|      34.1|         741270|           826172|         1567442|             61995|      205339|                  2.61|        PA|               Asian|122721|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|  Hispanic or Latino| 65162|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|American Indian a...|  8791|\n",
      "|      Fort Myers|       Florida|      37.3|          36850|            37165|           74015|              4312|       15365|                  2.45|        FL|               White| 50169|\n",
      "|      Pittsburgh|  Pennsylvania|      32.9|         149690|           154695|          304385|             17728|       28187|                  2.13|        PA|               White|208863|\n",
      "|          Laredo|         Texas|      28.8|         124305|           131484|          255789|              4921|       68427|                  3.66|        TX|American Indian a...|  1253|\n",
      "|        Berkeley|    California|      32.5|          60142|            60829|          120971|              3736|       25000|                  2.35|        CA|               Asian| 27089|\n",
      "|     Santa Clara|    California|      35.2|          63278|            62938|          126216|              4426|       52281|                  2.75|        CA|               White| 55847|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read airport data\n",
    "airport_df=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\").load(\"airport-codes_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read sas_data\n",
    "sas_df = spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sas_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read country data\n",
    "country_df=spark.read.format(\"csv\").option(\"header\", \"True\").option(\"delimiter\", \",\").load(\"countries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  code|             Country|\n",
      "+------+--------------------+\n",
      "|  582 |  'MEXICO Air Sea...|\n",
      "|  236 |       'AFGHANISTAN'|\n",
      "|  101 |           'ALBANIA'|\n",
      "+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read visa data\n",
    "visa_df=spark.read.format(\"csv\").option(\"header\", \"True\").option(\"delimiter\", \"=\").load(\"visa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|code|VisaType|\n",
      "+----+--------+\n",
      "|   1|Business|\n",
      "|   2|Pleasure|\n",
      "|   3| Student|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visa_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read I94mode data\n",
    "I94mode_df=spark.read.format(\"csv\").option(\"header\", \"True\").option(\"delimiter\", \",\").load(\"I94mode.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|code|        I94mode|\n",
      "+----+---------------+\n",
      "|   1|          'Air'|\n",
      "|   2|          'Sea'|\n",
      "|   3|         'Land'|\n",
      "|   9|'Not reported' |\n",
      "+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "I94mode_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### these operations need to be done:\n",
    "\n",
    "- city data may need \"string or float\" to \"integer\" format, also need to use sum aggregation to calculate the total population.\n",
    "\n",
    "- airport data need to process iso_region column to get the state information\n",
    "\n",
    "- sas_data need to change multiple columns data type to integer, also extract the arrive_date and split them into year, month and day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "cities_clean_df=cities_df.withColumn(\"Count\",cities_df[\"Count\"].cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "cities_clean_df = cities_clean_df.groupBy(\"City\", \"State\", \"Median Age\", \"Male Population\",\n",
    "                                     \"Female Population\" \\\n",
    "                                     , \"Total Population\", \"Number of Veterans\",\"Foreign-born\",\n",
    "                                     \"Average Household Size\", \"State Code\").agg(_sum(\"Count\").alias('Total Count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+-----------+\n",
      "|      City|         State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|Total Count|\n",
      "+----------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+-----------+\n",
      "|Manchester| New Hampshire|      37.3|          54845|            55378|          110223|              5473|       14506|                   2.4|        NH|     123828|\n",
      "| Charlotte|North Carolina|      34.3|         396646|           430475|          827121|             36046|      128897|                  2.52|        NC|     926239|\n",
      "|    Skokie|      Illinois|      43.4|          31382|            33437|           64819|              1066|       27424|                  2.78|        IL|      72441|\n",
      "+----------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities_clean_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airport_clean_df=airport_df.where((airport_df[\"iso_country\"]== \"US\")&airport_df['type'].isin([\"large_airport\", \"medium_airport\", \"small_airport\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "from pyspark.sql.functions import *\n",
    "airport_clean_df= airport_clean_df.withColumn(\"iso_region\", substring(airport_df[\"iso_region\"], 4, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|        KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|        AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|        AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_clean_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_clean_df=sas_df.withColumn(\"cicid\", sas_df[\"cicid\"].cast(\"integer\")) \\\n",
    "                   .withColumn(\"i94yr\", sas_df[\"i94yr\"].cast(\"integer\")) \\\n",
    "                   .withColumn(\"i94mon\", sas_df[\"i94mon\"].cast(\"integer\")) \\\n",
    "                   .withColumn(\"i94mode\", sas_df[\"i94mode\"].cast(\"integer\")) \\\n",
    "                   .withColumn(\"i94visa\", sas_df[\"i94visa\"].cast(\"integer\")) \\\n",
    "                   .withColumn(\"biryear\", sas_df[\"biryear\"].cast(\"integer\")) \\\n",
    "                   .withColumn(\"i94bir\", sas_df[\"i94bir\"].cast(\"integer\")) \\\n",
    "                   .withColumn(\"i94res\", sas_df[\"i94res\"].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_clean_df=sas_clean_df.withColumn(\"time_start\", to_date(lit(\"01/01/1960\"), \"MM/dd/yyyy\")) \\\n",
    "            .withColumn(\"arrive_date\", expr(\"date_add(time_start, arrdate)\")) \\\n",
    "            .withColumn(\"leave_date\", expr(\"date_add(time_start, depdate)\")) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_clean_df=sas_clean_df.drop(\"time_start\", \"arrdate\", \"depdate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_clean_df=sas_clean_df.withColumn(\"arrive_ymd\", split(col(\"arrive_date\"), \"-\")) \\\n",
    "            .withColumn(\"arrive_year\", col(\"arrive_ymd\")[0]) \\\n",
    "            .withColumn(\"arrive_month\", col(\"arrive_ymd\")[1]) \\\n",
    "            .withColumn(\"arrive_day\", col(\"arrive_ymd\")[2]).drop(\"arrive_ymd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+-----------+----------+-----------+------------+----------+\n",
      "|  cicid|i94yr|i94mon|i94cit|i94res|i94port|i94mode|i94addr|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|arrive_date|leave_date|arrive_year|arrive_month|arrive_day|\n",
      "+-------+-----+------+------+------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+-----------+----------+-----------+------------+----------+\n",
      "|5748517| 2016|     4| 245.0|   438|    LOS|      1|     CA|    40|      1|  1.0|20160430|     SYD| null|      G|      O|   null|      M|   1976|10292016|     F|  null|     QF|9.495387003E10|00011|      B1| 2016-04-30|2016-05-08|       2016|          04|        30|\n",
      "|5748518| 2016|     4| 245.0|   438|    LOS|      1|     NV|    32|      1|  1.0|20160430|     SYD| null|      G|      O|   null|      M|   1984|10292016|     F|  null|     VA|9.495562283E10|00007|      B1| 2016-04-30|2016-05-17|       2016|          04|        30|\n",
      "|5748519| 2016|     4| 245.0|   438|    LOS|      1|     WA|    29|      1|  1.0|20160430|     SYD| null|      G|      O|   null|      M|   1987|10292016|     M|  null|     DL|9.495640653E10|00040|      B1| 2016-04-30|2016-05-08|       2016|          04|        30|\n",
      "+-------+-----+------+------+------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+-----------+----------+-----------+------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sas_clean_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Tranform data:\n",
    "\n",
    "Transform demographics dataset grouping by state an calculate all the totals.\n",
    "Transform inmigration dataset on order to get arrival date in different columns (year, month, day) for partitioning the dataset.\n",
    "\n",
    "Generate Model (Star Schema):\n",
    "\n",
    "Create all dimensions in parquet.\n",
    "Create fact table in parquet particioned by year, month, day of th arrival date.\n",
    "Insert in fact table only items with dimension keys right. For integrity and consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Conceptual/Mapping\n",
    "- We use Star Schema for this data\n",
    "- There is another file named Data_dictionary.md show the fact table and dimension table content\n",
    "\n",
    "#### fact table: sas_fact\n",
    "#### dimension tables: airport, cities, country, I94mode, visa\n",
    "\n",
    "- Transform the cities data: groupby by state and get the totals for total population, male and demale polulation.\n",
    "- Transform the sas_data by join with other dimension tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# city data check the all the totals group by state \n",
    "cities_final_df = cities_clean_df.groupBy(cities_clean_df[\"State Code\"].alias(\"State_code\"), cities_clean_df[\"State\"]) \\\n",
    "                  .agg({\"Total Population\": \"sum\", \"Male Population\": \"sum\", \"Female Population\": \"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cities_final_df=cities_final_df.withColumnRenamed(\"sum(Male Population)\",\"sum_Male_Population\")\n",
    "cities_final_df=cities_final_df.withColumnRenamed(\"sum(Total Population)\",\"sum_Total_Population\")\n",
    "cities_final_df=cities_final_df.withColumnRenamed(\"sum(Female Population)\",\"sum_Female_Population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------------+--------------------+---------------------+\n",
      "|State_code|         State|sum_Male_Population|sum_Total_Population|sum_Female_Population|\n",
      "+----------+--------------+-------------------+--------------------+---------------------+\n",
      "|        MT|       Montana|            87707.0|            181294.0|              93587.0|\n",
      "|        NC|North Carolina|          1466105.0|           3060199.0|            1594094.0|\n",
      "|        MD|      Maryland|           627951.0|           1312129.0|             684178.0|\n",
      "+----------+--------------+-------------------+--------------------+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities_final_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_fact_df = sas_clean_df \\\n",
    "            .join(cities_final_df, sas_clean_df[\"i94addr\"] == cities_final_df[\"State_Code\"], \"leftsemi\") \\\n",
    "            .join(airport_clean_df, sas_clean_df[\"i94port\"] == airport_clean_df[\"local_code\"], \"leftsemi\") \\\n",
    "            .join(country_df, sas_clean_df[\"i94cit\"] == country_df[\"code\"], \"leftsemi\") \\\n",
    "            .join(visa_df, sas_clean_df[\"i94visa\"] == visa_df[\"code\"], \"leftsemi\") \\\n",
    "            .join(I94mode_df, sas_clean_df[\"i94mode\"] == I94mode_df[\"code\"], \"leftsemi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+-----------+----------+-----------+------------+----------+\n",
      "|  cicid|i94yr|i94mon|i94cit|i94res|i94port|i94mode|i94addr|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|arrive_date|leave_date|arrive_year|arrive_month|arrive_day|\n",
      "+-------+-----+------+------+------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+-----------+----------+-----------+------------+----------+\n",
      "|5748522| 2016|     4| 245.0|   464|    HHW|      1|     HI|    57|      2|  1.0|20160430|     ACK| null|      G|      O|   null|      M|   1959|10292016|     M|  null|     NZ|9.498180283E10|00010|      B2| 2016-04-30|2016-05-05|       2016|          04|        30|\n",
      "|5748523| 2016|     4| 245.0|   464|    HHW|      1|     HI|    66|      2|  1.0|20160430|     ACK| null|      G|      O|   null|      M|   1950|10292016|     F|  null|     NZ|9.497968993E10|00010|      B2| 2016-04-30|2016-05-12|       2016|          04|        30|\n",
      "|5748524| 2016|     4| 245.0|   464|    HHW|      1|     HI|    41|      2|  1.0|20160430|     ACK| null|      G|      O|   null|      M|   1975|10292016|     F|  null|     NZ|9.497974673E10|00010|      B2| 2016-04-30|2016-05-12|       2016|          04|        30|\n",
      "+-------+-----+------+------+------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+-----------+----------+-----------+------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sas_fact_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- save all the dataframe into parquet format, the fact table is saved with partion by year, month and day\n",
    "- read all the parquet tables and check the row number to make sure it is not empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# save each dataframe as parquet format\n",
    "\n",
    "# first save these dimension Tables\n",
    "cities_final_df.write.mode('overwrite').parquet(\"cities.parquet\")\n",
    "airport_clean_df.write.mode('overwrite').parquet(\"airport.parquet\")\n",
    "country_df.write.mode('overwrite').parquet(\"country.parquet\")\n",
    "visa_df.write.mode('overwrite').parquet(\"visa.parquet\")\n",
    "I94mode_df.write.mode('overwrite').parquet(\"I94mode.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_fact_df.write.partitionBy(\"arrive_year\", \"arrive_month\", \"arrive_day\").mode('overwrite').parquet(\"sas_fact.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "# read all the parquet files and check the row number'\n",
    "cities_read_df=spark.read.parquet(\"cities.parquet\")\n",
    "airport_read_df=spark.read.parquet(\"airport.parquet\")\n",
    "country_read_df=spark.read.parquet(\"country.parquet\")\n",
    "visa_read_df=spark.read.parquet(\"visa.parquet\")\n",
    "I94mode_read_df=spark.read.parquet(\"I94mode.parquet\")\n",
    "sas_fact_read_df=spark.read.parquet(\"sas_fact.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_read_df.count()>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_read_df.count()>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_read_df.count()>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visa_read_df.count()>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I94mode_read_df.count()>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sas_fact_read_df.count()>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- This project I use Spark to deal with the data and use parquet format to save the data, both of them can deal with large size of data. The data should be updated every day since each day we have new data\n",
    "\n",
    "- The data was increased by 100x, I think the current method still works.\n",
    "\n",
    "- The data populates a dashboard that must be updated on a daily basis by 7am every day, then we should use airflow to auto it.\n",
    "\n",
    "- The database needed to be accessed by 100+ people, the we may use Redshift, this AWS data warehouse can shupport 500 connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
